{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad717c-410d-491e-abff-4a7caf9acf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e2276-f742-4997-b798-89e9b1b1e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pdf file\n",
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "reader = PdfReader(\"AshlyLauResume.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text().replace(\"to\", \"To\").replace(\"•\", \"\")\n",
    "\n",
    "#text = remove_punctuation(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e2832a-36ec-4fd8-8391-ad00f3aa80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ivy Haddington\n",
    "Austin, TX | 512-555-0175 | ihaddington@email.com\n",
    "Summary\n",
    "&nbsp;\n",
    "Motivated Software Engineer with an exceptional educational background in computer science and technology. Proficient in essential software engineering skills, including full-stack development and network architecture.\n",
    "Education\n",
    "&nbsp;\n",
    "University of Los Angeles\n",
    "Bachelor of Science in computer science\n",
    "Experience\n",
    "&nbsp;\n",
    "Automated Intelligence Enterprises, Anchorage, AK, Programmer\n",
    "July 2019 – Current\n",
    "Use common programming languages, including HTML, C++ and Python\n",
    "Write and test code for computer programs\n",
    "Identify and correct coding errors\n",
    "Change code to reflect software updates\n",
    "Rewrite existing software programs for different operating systems\n",
    "KLD Development Corp, Los Angeles, CA, Project Manager\n",
    "May 2018 – July 2019\n",
    "Planned and organized 30+ IT projects\n",
    "Built teams and delegated project tasks\n",
    "Monitored project progress to ensure efficiency\n",
    "Communicated with clients about project specifications\n",
    "Updated project timeline as needed\n",
    "Certifications\n",
    "&nbsp;\n",
    "Certified Associate in Python Programming (PCAP)\n",
    "C++ Certified Associate Programmer (CPA)\n",
    "Skills\n",
    "&nbsp;\n",
    "Programming\n",
    "Communication\n",
    "Problem-solving\n",
    "Attention to detail\n",
    "Collaboration\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8332e058-03a8-45ac-b7ac-6bc5cec80f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashly Service\n",
      "['ashlylau@gmail.com']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    first_name = \"\"\n",
    "    last_name = \"\"\n",
    "    for token in nlp_text:\n",
    "        if token.pos_ == 'PROPN' and not token.is_stop:\n",
    "            \n",
    "            if first_name == \"\":\n",
    "                first_name = token.text\n",
    "            else:\n",
    "                last_name = token.text\n",
    "    \n",
    "    return f\"{first_name} {last_name}\"\n",
    "\n",
    "email_pattern = r'[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}'\n",
    "phone_pattern = r'(\\+\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[\\s.-]?)?\\d{3}[\\s.-]?\\d{4}'\n",
    "\n",
    "def extract_email(text):\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "def extract_phone_number(text):\n",
    "    return re.findall(phone_pattern, text)\n",
    "\n",
    "print(extract_name(text))\n",
    "print(extract_email(text))\n",
    "print(extract_phone_number(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61e5dc-fcfd-48b6-81d1-cff2bb9561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "059c6c72-8387-4344-ac7e-dc9c25349ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Extracting Degree:\n",
    "\n",
    "#Defines a function extract_degree to extract degrees (Bachelor, Master, PhD, Associate) from the text using regular expressions.\n",
    "import re\n",
    "\n",
    "def extract_degree(text):\n",
    "  pattern = r'(?=Bachelor|Master|Phd|Associate)(\\w+\\s).*'\n",
    "\n",
    "  matches = re.search(pattern, text)\n",
    "  if matches:\n",
    "    return matches.group(0)\n",
    "print(extract_degree(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15bd5401-b644-4022-b5d7-c62b529ed4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imperial College London']\n"
     ]
    }
   ],
   "source": [
    "#all universities https://www.usnews.com/education/best-global-universities/search?format=json&page=4\n",
    "\n",
    "#Extracting University Name:\n",
    "#Reads a CSV file containing a list of university names and defines a function find_university_name to extract university names from the text using regular expressions.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_universities.csv\")\n",
    "\n",
    "university_list = df[\"name\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_university_name(resume_text, universities):\n",
    "    found_universities = []\n",
    "    #resume_text = resume_text.lower()\n",
    "    for university in universities:\n",
    "        #university = university.replace(\"&\", \"and\")\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(university))  # Create a regex pattern for each university\n",
    "        if re.search(pattern, resume_text, re.IGNORECASE):\n",
    "            found_universities.append(university)\n",
    "    return found_universities # list of founded univ\n",
    "print(find_university_name(text, university_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6188da88-c3c6-4f7b-9e35-68cb1141e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'github', 'real-time', 'tensorflow', 'mathematics', 'gmail', 'c', 'operating systems', 'data analysis', 'r', 'html5', 'django', 'segmentation', 'international', 'postgresql', 'video', 'analysis', 'cisco', 'coding', 'ux', 'teaching', 'facebook', 'scheduling', 'database', 'engineering', 'physics', 'machine learning', 'marketing', 'java', 'data collection', 'javascript', 'ai', 'programming', 'workflow', 'mobile', 'economics', 'chemistry', 'filing', 'python', 'android'}\n"
     ]
    }
   ],
   "source": [
    "#Extracting Skills:\n",
    "\n",
    "#Reads a CSV file containing a list of skills and defines a function extract_skills to extract skills from the text using regular expressions.\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"skills.csv\")\n",
    "skills_list = df[\"0\"].tolist()\n",
    "\n",
    "def extract_skills(resume_text, skills):\n",
    "    found_skills = []\n",
    "    for university in skills:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(university))  # Create a regex pattern for each university\n",
    "        if re.search(pattern, resume_text, re.IGNORECASE):\n",
    "            found_skills.append(university.lower())\n",
    "    return set(found_skills)\n",
    "print(extract_skills(text, skills_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf19b604-da2c-4b32-9a38-5b0e82fae03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Intelligence Enterprises Anchorage AK Programmer July 2019 to Current Use common\n",
      "---KLD Development Corp Los Angeles CA May 2018 to July 2019\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from itertools import permutations\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from ordered_set import OrderedSet\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Download required NLTK resources\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Define helper functions\n",
    "\n",
    "#Text Preprocessing:\n",
    "#Removes punctuation, tokenizes the text, and performs part-of-speech tagging using NLTK.\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text and return the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"–\", \"to\").replace(\"-\", \"to\")\n",
    "    words = word_tokenize(text)\n",
    "    words_without_punctuation = [word for word in words if word not in string.punctuation]\n",
    "    return ' '.join(words_without_punctuation)\n",
    "\n",
    "def remove_verb_plus(text):\n",
    "    \"\"\"\n",
    "    Remove verbs and other specified parts of speech, returning cleaned text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    strr = \"\"\n",
    "    for i in pos_tags:\n",
    "        if i[1] in [\"NNP\", \"CD\", \"TO\"]:\n",
    "            strr += i[0] + \" \"\n",
    "    return strr\n",
    "\n",
    "def remove_repetitive_words(txt):\n",
    "  result = OrderedSet(txt.split(\" \"))\n",
    "  g = \" \".join(result)\n",
    "  return g\n",
    "\n",
    "#Extracting Organizations:\n",
    "#Uses spaCy's named entity recognition (NER) to extract organization names from the text.\n",
    "\n",
    "def extract_organizations(text):\n",
    "    \"\"\"\n",
    "    Extract organization names from the text using spaCy's named entity recognition (NER).\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    return organizations\n",
    "\n",
    "def find_org_in_text(text, orgs):\n",
    "    \"\"\"\n",
    "    Find and return organization names present in the given text.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for org in orgs:\n",
    "        if org in text:\n",
    "            new_list.append(org)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "\n",
    "#Searching for Patterns:\n",
    "#Defines a function search to search for specific patterns in the text based on part-of-speech tags.\n",
    "\n",
    "def search(pat):\n",
    "    \"\"\"\n",
    "    Search for patterns in the POS-tagged text and extract relevant information.\n",
    "    \"\"\"\n",
    "    M = len(pat)\n",
    "    N = len(pos_tags)\n",
    "\n",
    "    results = []\n",
    "    for i in range(N-M):\n",
    "        for j in range(M):\n",
    "            k = j + 1\n",
    "            if pos_tags[i+j][1] != pat[j]:\n",
    "                break\n",
    "        if k == M: # found pat tern\n",
    "\n",
    "            index = pat.index(\"CD\") # to split text into []\"CD\"[] , the left side will contain an unstrucutred organization , so we will format it with the list of orgs we have before\n",
    "\n",
    "            left_text = \" \".join([t[0] for t in pos_tags[i-5:i+index-1]])\n",
    "\n",
    "            orgs_found_in_left = find_org_in_text(remove_verb_plus(left_text), all_orgs_in_cv)\n",
    "\n",
    "            if len(orgs_found_in_left) == 0:\n",
    "                results.append(\" \".join([t[0] for t in pos_tags[i:i+M+1]]))\n",
    "            else:\n",
    "                orgs_found_in_left = sorted(orgs_found_in_left, key=len, reverse=True)[0]\n",
    "                final_text = orgs_found_in_left + \" \" + \" \".join([t[0] for t in pos_tags[i+index-1:i+M+1]])\n",
    "                results.append(final_text)\n",
    "\n",
    "            del pos_tags[i:i+M]#delete the found text to not get the same text / similar text with 2 different patterns , so we get unique results\n",
    "\n",
    "            N = len(pos_tags) #since we delete text , we should update the length\n",
    "            M = M - i\n",
    "    return results\n",
    "def remove_text_from_text(text1, text2):\n",
    "  text1_list = text1.split(\" \")\n",
    "  for item in text1_list:\n",
    "    text2 = text2.replace(item, \"\")\n",
    "  return text2\n",
    "\n",
    "#\n",
    "text = remove_punctuation(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(text.split())\n",
    "\n",
    "# Define patterns to search for in the text\n",
    "patterns = [\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\"),\n",
    "    (\"NNP(, \"\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NN\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NN\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\",\"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\",\"NNP\", \"CD\", \"HYPH\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"CD\", \"HYPH\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"CD\", \"HYPH\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"CD\", \":\", \"NN\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"NNP\"),\n",
    "\n",
    "    (\"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"NNP\"),\n",
    "]\n",
    "\n",
    "for i in range(len(patterns)-1):\n",
    "  patterns.append(patterns[i][::-1])\n",
    "\n",
    "# Sort patterns by length in descending order\n",
    "sorted_patterns = sorted(patterns, key=len, reverse=True)\n",
    "\n",
    "# Extract organization names from the text\n",
    "all_orgs_in_cv = extract_organizations(text)\n",
    "\n",
    "# Search for patterns in the POS-tagged text\n",
    "\n",
    "final_res = []\n",
    "for pp in sorted_patterns:\n",
    "\n",
    "    res = search(pp)\n",
    "    if len(res) != 0:\n",
    "      for r in res:\n",
    "        #s = remove_repetitive_words(r)\n",
    "        print(\"---\" + r)\n",
    "        text = remove_text_from_text(r, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6895664e-03c8-4396-960b-89ba50215b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Software Engineer at IBM from 2018 to 2022\"\n",
    "\n",
    "text = remove_punctuation(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26644bd7-a556-4dd1-ae3d-9aadcb19a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write efficient, maintainable and reusable code prioritizing privacy and security\n",
      "\n",
      "___\n",
      "Write technical documents, user guides and support documentation\n",
      "Adapt existing methods and procedures to create specialized solutions to intricate software problems\n",
      "\n",
      "___\n",
      "Developed front-end web UI and back-end web services\n",
      "River Tech,\n",
      "___\n",
      "Junior iOS Developer\n",
      "Aug. 2020 - Jul. 2021\n",
      "\n",
      "___\n",
      "Worked with scalable cloud services including Amazon S3 and Rackspace\n",
      "\n",
      "___\n",
      "Added e-commerce functionality to existing applications\n",
      "\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "#Extracting Key Achievements:\n",
    "#Uses spaCy for named entity recognition to extract key achievements from the text.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = sorted(text.split(\"Experience\"), key=len, reverse=True)[0]\n",
    "text = \"\"\"ane & Jenkins, iOS Developer\n",
    "Jul. 2021 - Current\n",
    "Write efficient, maintainable and reusable code prioritizing privacy and security\n",
    "Write technical documents, user guides and support documentation\n",
    "Adapt existing methods and procedures to create specialized solutions to intricate software problems\n",
    "Developed front-end web UI and back-end web services\n",
    "River Tech, Junior iOS Developer\n",
    "Aug. 2020 - Jul. 2021\n",
    "Worked with scalable cloud services including Amazon S3 and Rackspace\n",
    "Added e-commerce functionality to existing applications\n",
    "Collaborated with the team for architectural decisions\n",
    "Certifications\n",
    "&nbsp;\n",
    "\n",
    "\"\"\"\n",
    "for org in all_orgs_in_cv:\n",
    "  text = text.replace(org, \"\").replace(\"•\", \"\")\n",
    "doc = nlp(text)\n",
    "index = []\n",
    "\n",
    "for i in range(len(doc)-3):\n",
    "    if doc[i].pos_ == \"VERB\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"ADP\", \"VERB\", \"DET\"] and doc[i-1].text != \"and\" and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"ADJ\" and doc[i+2].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"ADP\" and doc[i+2].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "    if doc[i].pos_ == \"PRON\" and doc[i+1].pos_ == \"AUX\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i) # like : I was ..\n",
    "\n",
    "for i in range(len(index)-1):\n",
    "    print(doc[index[i]:index[i+1]])\n",
    "    print(\"___\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29e59e81-d6f6-4263-a74a-5cad4687e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assisted with the formulation of marketing strategies by working with a team To brief media and creative staff ·\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "    if doc[i].pos_ == \"VERB\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"ADP\", \"VERB\", \"DET\"]\n",
    "                             and doc[i-1].text != \"and\" \n",
    "                             and doc[i].text[0].isupper():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810c628-675d-4129-aef4-79de08ba242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Universities from Website:\n",
    "#Scrapes university names from the \"usnews.com\" website and saves them to a CSV file.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "resutls = []\n",
    "\n",
    "for i in range(217):\n",
    "  url = f'https://www.usnews.com/education/best-global-universities/search?format=json&page={i}'\n",
    "  headers = {\n",
    "      'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0'\n",
    "  }\n",
    "  response = requests.get(url, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "      print(\"Request successful\")\n",
    "      js = json.loads(response.text)\n",
    "      items = js.get(\"items\", \"\")\n",
    "      for i in items:\n",
    "        name = i.get(\"name\", \"\")\n",
    "        resutls.append(name)\n",
    "      # Process response here\n",
    "  else:\n",
    "      print(\"Request failed with status code:\", response.status_code)\n",
    "df = pd.DataFrame(resutls)\n",
    "df.to_csv(\"all_universities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e20e9-6445-4374-a8c8-e8bc508a25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using chatgpt\n",
    "\n",
    "#Uses Selenium to interact with a website (chatgpt.com) for further processing, likely for generating a report based on extracted information\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "# Initialize the undetected Chrome driver\n",
    "\n",
    "\n",
    "driver = uc.Chrome()\n",
    "prompt = '''extract this \"{\n",
    "  \"name\": \"\",\n",
    "  \"email\": \"\"\n",
    "  \"phone\": \"\",\n",
    "  \"education\": [\n",
    "    {\n",
    "      \"institution\": \"\",\n",
    "      \"location\": \"\",\n",
    "      \"year\": \"\"\n",
    "    }\n",
    "  ],\n",
    "  \"experience\": [\n",
    "    {\n",
    "      \"title\": \"\",\n",
    "      \"company\": \"\",\n",
    "      \"location\": \"\",\n",
    "      \"dates\": \"\",\n",
    "      \"short_description\": \"\",\n",
    "      \"key_acheviement\": \"\",\n",
    "    },]\n",
    "\"certificates\":\"\",\n",
    "\"skills\":[]\" from \"''' + text + '\", if some variables does not exist , put none'\n",
    "\n",
    "try:\n",
    "    # Open a webpage\n",
    "    driver.get('https://chatgpt.com/')\n",
    "    \n",
    "    time.sleep(10)\n",
    "    # Print the title of the webpage\n",
    "    text_area = driver.find_element(By.ID, \"prompt-textarea\")\n",
    "    text_area.send_keys(prompt)\n",
    "    text_area.send_keys(Keys.ENTER)\n",
    "    time.sleep(40)\n",
    "    code = driver.find_element(By.TAG_NAME, \"code\")\n",
    "    c = json.loads(code.text)\n",
    "    print(c)\n",
    "        \n",
    "    # Perform other actions with the driver...\n",
    "    \n",
    "finally:\n",
    "    # Close the driver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed3be38-3390-40fc-9fb2-7db8a7b456f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '', 'email': '', 'phone': '', 'education': [{'institution': '', 'location': '', 'year': ''}], 'experience': [{'title': '', 'company': '', 'location': '', 'dates': '', 'short_description': '', 'key_acheviement': ''}], 'certificates': '', 'skills': []}\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "# Initialize the undetected Chrome driver\n",
    "driver = uc.Chrome()\n",
    "try:\n",
    "    driver.get('https://chatgpt.com/')\n",
    "    \n",
    "    time.sleep(10)\n",
    "    text_area = driver.find_element(By.ID, \"prompt-textarea\")\n",
    "    text_area.send_keys(prompt)\n",
    "    text_area.send_keys(Keys.ENTER)\n",
    "    time.sleep(40)\n",
    "    code = driver.find_element(By.TAG_NAME, \"code\")\n",
    "    c = json.loads(code.text)\n",
    "    print(c)\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6426400-e482-4d99-8abf-96f9a8043304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/a-mstr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/a-mstr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc4 in position 10: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), filename)):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 26\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m             sk \u001b[38;5;241m=\u001b[39m extract_skills(content)\n",
      "File \u001b[0;32m/usr/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc4 in position 10: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import nltk\n",
    "from itertools import permutations\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def extract_skills(text_html):\n",
    "    soup = BeautifulSoup(text_html, 'lxml')\n",
    "    t = soup.find(attrs={\"data-section-cd\":\"HILT\"})\n",
    "    return t.text\n",
    "\n",
    "skills = []\n",
    "\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if os.path.isfile(os.path.join(os.getcwd(), filename)):\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "            try:\n",
    "                sk = extract_skills(content)\n",
    "                skills.append(sk)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# Define helper functions\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text and return the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"–\", \"to\").replace(\"-\", \"to\")\n",
    "    words = word_tokenize(text)\n",
    "    words_without_punctuation = [word for word in words if word not in string.punctuation]\n",
    "    return ' '.join(words_without_punctuation)\n",
    "\n",
    "new_list = set()\n",
    "for ele in mylist:\n",
    "    if len(ele) <= 50:\n",
    "        ele = remove_punctuation(ele)\n",
    "        for sub_ele in ele.split():\n",
    "            new_list.add(sub_ele)\n",
    "\n",
    "\n",
    "g = pandas.DataFrame(new_list)\n",
    "g.to_csv(\"/home/a-mstr/Resume/all_skills_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720225a2-0483-480c-9ef5-b71707701b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_spacy_doc(file, data):\n",
    "    nlp = spacy.blank('en')\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        annot = annot['entities']\n",
    "        ents = []\n",
    "        entity_indices = []\n",
    "        skip_entity = False\n",
    "        for start, end, label in annot:\n",
    "            skip_entity = False\n",
    "            for idx in range(start, end):\n",
    "                if idx in entity_indices:\n",
    "                    skip_entity = True\n",
    "                    break\n",
    "            if skip_entity: continue\n",
    "            entity_indices = entity_indices + list(range(start, end))\n",
    "            try:\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "                continue\n",
    "                if span is None:\n",
    "                    err_data = str([start, end]) + \" \" + str(text) + \"\\n\"\n",
    "                    file.write(err_data)\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "            try:\n",
    "                doc.ents = ents\n",
    "                db.add(doc)\n",
    "            except:\n",
    "                pass\n",
    "    return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffa165-ce74-4ffc-ac98-c85515441c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cv_data, test_size=0.3)\n",
    "file = open('error.txt', 'w', encoding='utf-8')\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_data.spacy')\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_data.spacy')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a5bfe-3fab-4742-abff-bc67afb2f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
