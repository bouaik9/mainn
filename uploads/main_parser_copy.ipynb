{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad717c-410d-491e-abff-4a7caf9acf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e2276-f742-4997-b798-89e9b1b1e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pdf file\n",
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "reader = PdfReader(\"AshlyLauResume.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text().replace(\"to\", \"To\").replace(\"•\", \"\")\n",
    "\n",
    "#text = remove_punctuation(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8332e058-03a8-45ac-b7ac-6bc5cec80f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashly Service\n",
      "['ashlylau@gmail.com']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    first_name = \"\"\n",
    "    last_name = \"\"\n",
    "    for token in nlp_text:\n",
    "        if token.pos_ == 'PROPN' and not token.is_stop:\n",
    "            \n",
    "            if first_name == \"\":\n",
    "                first_name = token.text\n",
    "            else:\n",
    "                last_name = token.text\n",
    "    \n",
    "    return f\"{first_name} {last_name}\"\n",
    "\n",
    "email_pattern = r'[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}'\n",
    "phone_pattern = r'(\\+\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[\\s.-]?)?\\d{3}[\\s.-]?\\d{4}'\n",
    "\n",
    "def extract_email(text):\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "def extract_phone_number(text):\n",
    "    return re.findall(phone_pattern, text)\n",
    "\n",
    "print(extract_name(text))\n",
    "print(extract_email(text))\n",
    "print(extract_phone_number(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059c6c72-8387-4344-ac7e-dc9c25349ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Extracting Degree:\n",
    "\n",
    "#Defines a function extract_degree to extract degrees (Bachelor, Master, PhD, Associate) from the text using regular expressions.\n",
    "import re\n",
    "\n",
    "def extract_degree(text):\n",
    "  pattern = r'(?=Bachelor|Master|Phd|Associate)(\\w+\\s).*'\n",
    "\n",
    "  matches = re.search(pattern, text)\n",
    "  if matches:\n",
    "    return matches.group(0)\n",
    "print(extract_degree(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bd5401-b644-4022-b5d7-c62b529ed4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imperial College London']\n"
     ]
    }
   ],
   "source": [
    "#all universities https://www.usnews.com/education/best-global-universities/search?format=json&page=4\n",
    "\n",
    "#Extracting University Name:\n",
    "#Reads a CSV file containing a list of university names and defines a function find_university_name to extract university names from the text using regular expressions.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_universities.csv\")\n",
    "\n",
    "university_list = df[\"name\"].tolist()\n",
    "\n",
    "\n",
    "def find_university_name(resume_text, universities):\n",
    "    found_universities = []\n",
    "    #resume_text = resume_text.lower()\n",
    "    for university in universities:\n",
    "        #university = university.replace(\"&\", \"and\")\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(university))  # Create a regex pattern for each university\n",
    "        if re.search(pattern, resume_text, re.IGNORECASE):\n",
    "            found_universities.append(university)\n",
    "    return found_universities # list of founded univ\n",
    "print(find_university_name(text, university_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6188da88-c3c6-4f7b-9e35-68cb1141e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'postgresql', 'economics', 'html5', 'facebook', 'javascript', 'database', 'chemistry', 'cisco', 'marketing', 'c', 'engineering', 'mobile', 'operating systems', 'workflow', 'scheduling', 'teaching', 'segmentation', 'real-time', 'machine learning', 'django', 'java', 'physics', 'ux', 'github', 'gmail', 'analysis', 'mathematics', 'r', 'data analysis', 'tensorflow', 'python', 'android', 'coding', 'data collection', 'video', 'international', 'ai', 'filing', 'programming'}\n"
     ]
    }
   ],
   "source": [
    "#Extracting Skills:\n",
    "\n",
    "#Reads a CSV file containing a list of skills and defines a function extract_skills to extract skills from the text using regular expressions.\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"skills.csv\")\n",
    "skills_list = df[\"0\"].tolist()\n",
    "\n",
    "def extract_skills(resume_text, skills):\n",
    "    found_skills = []\n",
    "    for university in skills:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(university))  # Create a regex pattern for each university\n",
    "        if re.search(pattern, resume_text, re.IGNORECASE):\n",
    "            found_skills.append(university.lower())\n",
    "    return set(found_skills)\n",
    "print(extract_skills(text, skills_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf19b604-da2c-4b32-9a38-5b0e82fae03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Raffles Institution 01/2015 to 12/2016 CambridgetoSingapore GCE A Level · 7\n",
      "---EXPERIENCE Google 07/2019 to 09/2019 Summer Trainee Engineering Practicum Intern ·\n",
      "---Android Bootstrap 3 TensorFlow AWARDS/ACHIEVEMENTS ·\n",
      "---Singapore National Team Artistic Gymnastics 01/2012 to 12/2016 · Participated\n",
      "---Glasgow Commonwealth Games PROJECTS CHARJE 10/2019 to 01/2020 · Implemented\n",
      "---DDB Worldwide Pte Ltd 02/2017 to 04/2017 Account Executive Intern\n",
      "---Android Council 01/2015 to 12/2016 · EAGLES\n",
      "---HP Inc. 07/2018 to 08/2018 Software R D Intern ·\n",
      "---Imperial College London 09/2017 to 06/2021 Computing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from itertools import permutations\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from ordered_set import OrderedSet\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Download required NLTK resources\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Define helper functions\n",
    "\n",
    "#Text Preprocessing:\n",
    "#Removes punctuation, tokenizes the text, and performs part-of-speech tagging using NLTK.\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text and return the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"–\", \"to\").replace(\"-\", \"to\")\n",
    "    words = word_tokenize(text)\n",
    "    words_without_punctuation = [word for word in words if word not in string.punctuation]\n",
    "    return ' '.join(words_without_punctuation)\n",
    "\n",
    "def remove_verb_plus(text):\n",
    "    \"\"\"\n",
    "    Remove verbs and other specified parts of speech, returning cleaned text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    strr = \"\"\n",
    "    for i in pos_tags:\n",
    "        if i[1] in [\"NNP\", \"CD\", \"TO\"]:\n",
    "            strr += i[0] + \" \"\n",
    "    return strr\n",
    "\n",
    "def remove_repetitive_words(txt):\n",
    "  result = OrderedSet(txt.split(\" \"))\n",
    "  g = \" \".join(result)\n",
    "  return g\n",
    "\n",
    "#Extracting Organizations:\n",
    "#Uses spaCy's named entity recognition (NER) to extract organization names from the text.\n",
    "\n",
    "def extract_organizations(text):\n",
    "    \"\"\"\n",
    "    Extract organization names from the text using spaCy's named entity recognition (NER).\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    return organizations\n",
    "\n",
    "def find_org_in_text(text, orgs):\n",
    "    \"\"\"\n",
    "    Find and return organization names present in the given text.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for org in orgs:\n",
    "        if org in text:\n",
    "            new_list.append(org)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "\n",
    "#Searching for Patterns:\n",
    "#Defines a function search to search for specific patterns in the text based on part-of-speech tags.\n",
    "\n",
    "def search(pat):\n",
    "    \"\"\"\n",
    "    Search for patterns in the POS-tagged text and extract relevant information.\n",
    "    \"\"\"\n",
    "    M = len(pat)\n",
    "    N = len(pos_tags)\n",
    "\n",
    "    results = []\n",
    "    for i in range(N-M):\n",
    "        for j in range(M):\n",
    "            k = j + 1\n",
    "            if pos_tags[i+j][1] != pat[j]:\n",
    "                break\n",
    "        if k == M: # found pat tern\n",
    "\n",
    "            index = pat.index(\"CD\") # to split text into []\"CD\"[] , the left side will contain an unstrucutred organization , so we will format it with the list of orgs we have before\n",
    "\n",
    "            left_text = \" \".join([t[0] for t in pos_tags[i-5:i+index-1]])\n",
    "\n",
    "            orgs_found_in_left = find_org_in_text(remove_verb_plus(left_text), all_orgs_in_cv)\n",
    "\n",
    "            if len(orgs_found_in_left) == 0:\n",
    "                results.append(\" \".join([t[0] for t in pos_tags[i:i+M+1]]))\n",
    "            else:\n",
    "                orgs_found_in_left = sorted(orgs_found_in_left, key=len, reverse=True)[0]\n",
    "                final_text = orgs_found_in_left + \" \" + \" \".join([t[0] for t in pos_tags[i+index-1:i+M+1]])\n",
    "                results.append(final_text)\n",
    "\n",
    "            del pos_tags[i:i+M]#delete the found text to not get the same text / similar text with 2 different patterns , so we get unique results\n",
    "\n",
    "            N = len(pos_tags) #since we delete text , we should update the length\n",
    "            M = M - i\n",
    "    return results\n",
    "def remove_text_from_text(text1, text2):\n",
    "  text1_list = text1.split(\" \")\n",
    "  for item in text1_list:\n",
    "    text2 = text2.replace(item, \"\")\n",
    "  return text2\n",
    "\n",
    "#\n",
    "text = remove_punctuation(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(text.split())\n",
    "\n",
    "# Define patterns to search for in the text\n",
    "patterns = [\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\"),\n",
    "    (\"NNP(, \"\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NN\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NN\", \"NNP\", \"NNP\", \"CD\", \":\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\",\"NNP\", \"CD\", \"TO\", \"CD\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\",\"NNP\", \"CD\", \"HYPH\", \"CD\"),\n",
    "    (\"NNP\", \"NNP\", \"CD\", \"HYPH\", \"NNP\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"CD\", \"HYPH\", \"CD\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"CD\", \":\", \"NN\", \"NNP\", \"NNP\"),\n",
    "    (\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"NNP\"),\n",
    "\n",
    "    (\"NNP\", \"NNP\", \"CD\", \"TO\", \"NNP\", \"NNP\"),\n",
    "]\n",
    "\n",
    "for i in range(len(patterns)-1):\n",
    "  patterns.append(patterns[i][::-1])\n",
    "\n",
    "# Sort patterns by length in descending order\n",
    "sorted_patterns = sorted(patterns, key=len, reverse=True)\n",
    "\n",
    "# Extract organization names from the text\n",
    "all_orgs_in_cv = extract_organizations(text)\n",
    "\n",
    "# Search for patterns in the POS-tagged text\n",
    "\n",
    "final_res = []\n",
    "for pp in sorted_patterns:\n",
    "\n",
    "    res = search(pp)\n",
    "    if len(res) != 0:\n",
    "      for r in res:\n",
    "        #s = remove_repetitive_words(r)\n",
    "        print(\"---\" + r)\n",
    "        text = remove_text_from_text(r, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26644bd7-a556-4dd1-ae3d-9aadcb19a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Key Achievements:\n",
    "#Uses spaCy for named entity recognition to extract key achievements from the text.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = sorted(text.split(\"Experience\"), key=len, reverse=True)[0]\n",
    "for org in all_orgs_in_cv:\n",
    "  text = text.replace(org, \"\").replace(\"•\", \"\")\n",
    "doc = nlp(text)\n",
    "index = []\n",
    "\n",
    "for i in range(len(doc)-3):\n",
    "    if doc[i].pos_ == \"VERB\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"ADP\", \"VERB\", \"DET\"] and doc[i-1].text != \"and\" and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"ADJ\" and doc[i+2].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "    if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"ADP\" and doc[i+2].pos_ == \"NOUN\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i)\n",
    "    if doc[i].pos_ == \"PRON\" and doc[i+1].pos_ == \"AUX\" and doc[i-1].pos_ not in [\"PART\", \"PRON\", \"DET\", \"ADP\", \"VERB\"] and doc[i].text[0].isupper():\n",
    "        index.append(i) # like : I was ..\n",
    "\n",
    "for i in range(len(index)-1):\n",
    "    print(doc[index[i]:index[i+1]])\n",
    "    print(\"___\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810c628-675d-4129-aef4-79de08ba242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Universities from Website:\n",
    "\n",
    "#Scrapes university names from the \"usnews.com\" website and saves them to a CSV file.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "resutls = []\n",
    "\n",
    "for i in range(217):\n",
    "  url = f'https://www.usnews.com/education/best-global-universities/search?format=json&page={i}'\n",
    "  headers = {\n",
    "      'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0'\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "      print(\"Request successful\")\n",
    "      js = json.loads(response.text)\n",
    "      items = js.get(\"items\", \"\")\n",
    "      for i in items:\n",
    "        name = i.get(\"name\", \"\")\n",
    "        resutls.append(name)\n",
    "      # Process response here\n",
    "  else:\n",
    "      print(\"Request failed with status code:\", response.status_code)\n",
    "\n",
    "df = pd.DataFrame(resutls)\n",
    "df.to_csv(\"all_universities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277367e-63bd-43b6-a24d-c46e81721569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting skills from dataset:\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import nltk\n",
    "from itertools import permutations\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def extract_skills(text_html):\n",
    "    soup = BeautifulSoup(text_html, 'lxml')\n",
    "    t = soup.find(attrs={\"data-section-cd\":\"HILT\"})\n",
    "    return t.text\n",
    "\n",
    "skills = []\n",
    "\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if os.path.isfile(os.path.join(os.getcwd(), filename)):\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "            try:\n",
    "                sk = extract_skills(content)\n",
    "                skills.append(sk)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# Define helper functions\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text and return the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"–\", \"to\").replace(\"-\", \"to\")\n",
    "    words = word_tokenize(text)\n",
    "    words_without_punctuation = [word for word in words if word not in string.punctuation]\n",
    "    return ' '.join(words_without_punctuation)\n",
    "\n",
    "new_list = set()\n",
    "for ele in skills:\n",
    "    if len(ele) <= 50:\n",
    "        ele = remove_punctuation(ele)\n",
    "        for sub_ele in ele.split():\n",
    "            new_list.add(sub_ele)\n",
    "\n",
    "\n",
    "g = pandas.DataFrame(new_list)\n",
    "g.to_csv(\"all_skills.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7e20e9-6445-4374-a8c8-e8bc508a25f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'shly Lau', 'email': 'ashlylau@gmail.com', 'phone': '', 'education': [{'institution': 'EUCTION', 'location': '', 'year': '09/201'}, {'institution': 'Cambridge', 'location': '', 'year': '0/2019'}], 'experience': [{'title': 'Account', 'company': '', 'location': '', 'dates': '02/201 - 04/201', 'short_description': 'ssisted with the formulation of marketing strategies by working with a team To brief media and creative staff  Facilitated communications between clients and creatives by managing client accounts and projects', 'key_acheviement': ''}], 'certificates': '', 'skills': ['Java', 'Python', 'Haskell', 'C', 'C++', 'JavaScript', 'Prolog', 'HTML5', 'CSS', 'Android Development', 'Web Development', 'GitHub', 'React Native']}\n"
     ]
    }
   ],
   "source": [
    "#Using chatgpt\n",
    "\n",
    "#Uses Selenium to interact with a website (chatgpt.com) for further processing, likely for generating a report based on extracted information\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "# Initialize the undetected Chrome driver\n",
    "\n",
    "\n",
    "driver = uc.Chrome()\n",
    "prompt = '''\n",
    "extract \"{  \"name\": \"\",\"email\": \"\" \"phone\": \"\", \"education\": [{\"institution\": \"\", \"location\": \"\", \"year\": \"\"  }],\"experience\": [  { \"title\": \"\", \"company\": \"\",\"location\": \"\",\"dates\": \"\",\"short_description\": \"\", \"key_acheviement\": \"\", },]\"certificates\":\"\",\"skills\":[]\" from \"''' + text + '\" in json format , if some variables does not exist , put none'\n",
    "\n",
    "try:\n",
    "    driver.get('https://chatgpt.com/')\n",
    "    \n",
    "    time.sleep(10)\n",
    "    # Print the title of the webpage\n",
    "    text_area = driver.find_element(By.ID, \"prompt-textarea\")\n",
    "    text_area.send_keys(prompt)\n",
    "    time.sleep(4)\n",
    "    text_area.send_keys(Keys.ENTER)\n",
    "    time.sleep(40)\n",
    "    code = driver.find_element(By.TAG_NAME, \"code\")\n",
    "    c = json.loads(code.text)\n",
    "    print(c)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab149fd-9fcd-44be-bf0b-2a276377ce23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
